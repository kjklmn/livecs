1.为什么要编解码？
编码：如果录制视频的时候屏幕的分辨率是100*100，那么一帧数据就是100*100个像素，也就是10000个像素，按照YUV420P每个像素占12bit计算，那么10000个像素多大？也就是120000bits，即15000个字节，15k。
如果一秒钟25帧，那么15K*25=375k,一分钟的视频就是375k*60=22500K=22.5m。但是，实际上现在很少录屏的分辨率是100*100，如果我的手机分辨率是1920*1080，那么我的手机录屏一分钟的文件大小是22.5*1920*1080/10000=4665.6m,
而且这还是YUV420P方式来录屏的，每个像素才占了12bit，也就是1.5byte。如果是RGB方式每个像素3个字节，那么视频文件的大小则会大一倍。而且我们只是计算了视频的，还有音频的我们还没算进去。这是一个大的可怕的数据。
所以无论是把视频保存到本地还是推流到流媒体服务器，都要进行压缩，也就是编码来缩小文件所占的大小，来降低推流时所占用的带宽和流量。

解码：几乎所有的视频格式，都是基于YCbCr（YUV）模型的。播放的时候，播放器需要将YCbCr的信息，通过计算，转换为RGB，这个步骤称为渲染。
除非你不做渲染，但是那怎么可能。就算不做渲染面板也不接受YUV信号，只接受转换后的RGB信号。
所以几乎所有的播放器，包括电视（YUV很好的兼容了黑白电视和彩色电视，黑白直接用Y也就是亮度，彩色电视YUV，色彩和亮度），显示器，还有我们软件开发中的播放器，ijkplayer，ffplayer等等都会进行渲染，都会把YUV转换成RGB显示出来。

2.音视频采集：
YUV码流的存储格式其实与其采样的方式密切相关，主流的采样方式有三种，YUV4:4:4，YUV4:2:2，YUV4:2:0，如何根据其采样格式来从码流中还原每个像素点的YUV值，因为只有正确地还原了每个像素点的YUV值，才能通过YUV与RGB的转换公式提取出每个像素点的RGB值，然后显示出来。
音视频采集（麦克风<音频采样数据：pcm>，相机<视频像素数据：YUV420p>）
音频：点击开始之后就回去开一个子线程开启audiorecord.startrecord(),然后audiorecord.read(buffer,0,buffer.length)来读取音频采样数据，然后传给ndk进行编码
视频：打开相机，通过预览方式，每预览一帧画面，通过回调获取预览的视频像素数据，然后给到ndk层进行编码
 

3.编码：
先给音频和视频编码器分别设置相应的参数，音频的要设置音频采样率（44100）和声道个数（1,2,...）
编码之后是推流还是保存本地？
如果是推流的话使用faac和x264的库来编码，使用rtmp的库来推流
我们先需要通过rtmp的库提供的api来和流媒体服务器建立连接：RTMP_SetupURL(rtmp, rtmp_path)。
然后通过faac和x264的库提供的API来编码。视频编码器是要把每一帧YUV420P视频数据编码成NUL数组，如果预览的数据不是YUV420P,则需要先把数据转换成YUV420P后再来编码。音频则把数据编码成字节数组。无论是音频还是视频，视频编码之后我们把这个NAL数组中的每一个元素，即每一个NAL数据包装成RTMPPacket放入一个RTMPPacket的队列中。音频则把数据也封装成RTMPPacket放入同一个RTMPPacket的队列中。然后消费者线程不断地从队列中取出RTMPPacket，通过API RTMP_SendPacket(rtmp, packet, TRUE)不断的把数据推流到流媒体服务器。
如果是保存本地的话：我们使用ffmpeg一个开源库就行了，因为后面会用到合并音视频文件.h264和.aac文件为一个.mp4文件，这个需要ffmpeg来实现。
点击开始录制的图标的时在C++中就启动了两个线程，其一是视频解码，另一个是音频解码，这两个线程分别运行编码函数startEncode（音视频编码的函数名都是这），此函数不断的从安全队列threadsafe_queue中获取每帧数据来编码，点击结束的时候通过
ffmpeg的命令行函数来合并音视频文件.h264和.aac文件为一个.mp4文件。

4.如果第3步的操作是推流的话，那么在客户端就需要做拉流的操作。拉流就是
 
 
 x264_picture_t 结构体描述一帧视频的特征，该结构体定义在x264.h中。
 typedef struct
 {
	...
	x264_image_t     img;            // 存放一帧图像的真实数据
	...
 }x264_picture_t
 
 
 x264_image_t 结构用于存放一帧图像实际像素数据。该结构体定义在x264.h中
 typedef struct
{
    int     i_csp;       //色彩空间参数 ，X264只支持I420
    int     i_stride[4]; //对应于各个色彩分量的跨度
    uint8_t *plane[4];   //对应于各个色彩分量的数据,每个图像平面存放数据的起始地址, plane[0]是Y平面，// plane[1]和plane[2]分别代表U和V平面
} x264_image_t;



typedef struct

{

int  i_ref_idc;        // Nal的优先级

int  i_type;           // Nal的类型

int  b_long_startcode; // 是否采用长前缀码0x00000001

int  i_first_mb;       // 如果Nal为一条带，则表示该条带第一个宏块的指数

    int  i_last_mb;        // 如果Nal为一条带，则表示该条带最后一个宏块的指数


    int  i_payload;        // payload 的字节大小
    uint8_t *p_payload;    // 存放编码后的数据，已经封装成Nal单元

} x264_nal_t;


